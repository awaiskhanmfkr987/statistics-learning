<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Theory of Statistical Tests</title>
<style>
body{font-family:Arial,Helvetica,sans-serif;background:#f8fafc;margin:0;line-height:1.9}
nav{background:#1e40af;padding:12px;text-align:center}
nav a{color:white;margin:12px;text-decoration:none;font-weight:bold}
section{max-width:1150px;margin:20px auto;background:white;padding:35px;border-radius:10px}
h1,h2,h3{color:#1e3a8a}
code{background:#eef2ff;padding:4px 6px;border-radius:4px}
pre{background:#f1f5f9;padding:12px;border-radius:6px;overflow:auto}
table{border-collapse:collapse;width:100%}
th,td{border:1px solid #cbd5e1;padding:8px;text-align:left}
th{background:#e0e7ff}
</style>
</head>
<body>

<nav>
<a href="index.html">Home</a>
<a href="learn.html">Learn</a>
<a href="calculator.html">Calculator</a>
<a href="theory.html">Theory</a>
</nav>

<section>

<h1>ğŸ“— Comprehensive Theory of Statistical Tests</h1>

<hr>

<h2>I. Foundational Concepts</h2>

<h3>1. Hypothesis Testing Framework</h3>
<p>
Statistical hypothesis testing is a formal decision-making framework used to assess evidence against a null model.
</p>

<ul>
<li><b>Null Hypothesis (Hâ‚€):</b> No effect or no difference</li>
<li><b>Alternative Hypothesis (Hâ‚):</b> Effect or difference exists</li>
<li><b>Significance Level (Î±):</b> Probability of Type I error (usually 0.05)</li>
<li><b>p-value:</b> Probability of observing data as extreme as sample, given Hâ‚€</li>
<li><b>Test Statistic:</b> Function of sample data with known distribution under Hâ‚€</li>
</ul>

<h3>Decision Rules</h3>
<ul>
<li>Reject Hâ‚€ if p-value &lt; Î±</li>
<li>Reject Hâ‚€ if test statistic exceeds critical value</li>
<li>Reject Hâ‚€ if null value not in confidence interval</li>
</ul>

<h3>2. Errors and Power</h3>
<ul>
<li><b>Type I Error (Î±):</b> Rejecting true Hâ‚€</li>
<li><b>Type II Error (Î²):</b> Failing to reject false Hâ‚€</li>
<li><b>Power (1âˆ’Î²):</b> Ability to detect true effects</li>
</ul>

<hr>

<h2>II. Parametric Tests â€“ Theoretical Foundations</h2>

<h3>1. t-Tests Family</h3>

<p>
t-tests are based on the <b>Studentâ€™s t-distribution</b>, which accounts for uncertainty in estimating population variance.
</p>

<h4>Properties of t-distribution</h4>
<ul>
<li>Symmetric, bell-shaped</li>
<li>Heavier tails than normal distribution</li>
<li>Approaches normal distribution as df â†’ âˆ</li>
</ul>

<h4>One-Sample t-Test</h4>
<pre>
t = (xÌ„ âˆ’ Î¼â‚€) / (s / âˆšn)
df = n âˆ’ 1
</pre>

<h4>Independent Samples t-Test</h4>
<b>Pooled Variance Case:</b>
<pre>
t = (xÌ„â‚ âˆ’ xÌ„â‚‚) / sâ‚šâˆš(1/nâ‚ + 1/nâ‚‚)
df = nâ‚ + nâ‚‚ âˆ’ 2
</pre>

<b>Welchâ€™s t-Test:</b>
<pre>
t = (xÌ„â‚ âˆ’ xÌ„â‚‚) / âˆš(sâ‚Â²/nâ‚ + sâ‚‚Â²/nâ‚‚)
</pre>

<h4>Paired t-Test</h4>
<pre>
t = dÌ„ / (s_d / âˆšn)
df = n âˆ’ 1
</pre>

<hr>

<h3>2. Analysis of Variance (ANOVA)</h3>

<p>
ANOVA partitions total variability into between-group and within-group components.
</p>

<h4>One-Way ANOVA Model</h4>
<pre>
Xáµ¢â±¼ = Î¼ + Ï„áµ¢ + Îµáµ¢â±¼
</pre>

<h4>Variance Decomposition</h4>
<pre>
SST = SSB + SSW
F = MSB / MSW
</pre>

<h4>ANOVA Assumptions</h4>
<ul>
<li>Independence</li>
<li>Normality of residuals</li>
<li>Homogeneity of variances</li>
</ul>

<h4>Extensions</h4>
<ul>
<li>Two-Way ANOVA (with interaction)</li>
<li>Repeated Measures ANOVA</li>
<li>MANOVA</li>
<li>ANCOVA</li>
</ul>

<hr>

<h3>3. Correlation and Regression</h3>

<h4>Pearson Correlation</h4>
<pre>
r = Î£[(xáµ¢âˆ’xÌ„)(yáµ¢âˆ’È³)] / âˆš[Î£(xáµ¢âˆ’xÌ„)Â² Î£(yáµ¢âˆ’È³)Â²]
</pre>

<h4>Simple Linear Regression</h4>
<pre>
Y = Î²â‚€ + Î²â‚X + Îµ
Î²Ì‚ = (X'X)â»Â¹X'Y
</pre>

<hr>

<h2>III. Non-Parametric Tests â€“ Theory</h2>

<h3>1. Rank-Based Tests</h3>

<h4>Mannâ€“Whitney U Test</h4>
<pre>
U = min(Uâ‚, Uâ‚‚)
z = (U âˆ’ Î¼_U) / Ïƒ_U
</pre>

<h4>Wilcoxon Signed-Rank Test</h4>
<pre>
T = min(Tâº, Tâ»)
</pre>

<h4>Kruskalâ€“Wallis Test</h4>
<pre>
H = [12/(N(N+1))] Î£(Ráµ¢Â²/náµ¢) âˆ’ 3(N+1)
</pre>

<hr>

<h3>2. Chi-Square Tests</h3>

<h4>Goodness of Fit</h4>
<pre>
Ï‡Â² = Î£ (O âˆ’ E)Â² / E
</pre>

<h4>Test of Independence</h4>
<pre>
df = (r âˆ’ 1)(c âˆ’ 1)
</pre>

<h4>Fisherâ€™s Exact Test</h4>
<p>
Used for small samples based on hypergeometric distribution.
</p>

<hr>

<h3>3. Rank Correlation</h3>

<h4>Spearmanâ€™s Ï</h4>
<pre>
Ï = 1 âˆ’ 6Î£dÂ² / [n(nÂ² âˆ’ 1)]
</pre>

<h4>Kendallâ€™s Ï„</h4>
<pre>
Ï„ = (C âˆ’ D) / [n(n âˆ’ 1)/2]
</pre>

<hr>

<h2>IV. Comparative Theory</h2>

<table>
<tr>
<th>Aspect</th>
<th>Parametric</th>
<th>Non-Parametric</th>
</tr>
<tr>
<td>Efficiency</td>
<td>High (assumptions met)</td>
<td>Slightly lower</td>
</tr>
<tr>
<td>Robustness</td>
<td>Low</td>
<td>High</td>
</tr>
<tr>
<td>Outliers</td>
<td>Sensitive</td>
<td>Resistant</td>
</tr>
</table>

<hr>

<h2>V. Multiple Testing Theory</h2>

<ul>
<li><b>Bonferroni:</b> Î±/m</li>
<li><b>Holmâ€™s Method:</b> Step-down correction</li>
<li><b>Benjamini-Hochberg:</b> Controls FDR</li>
</ul>

<hr>

<h2>VI. Bayesian & Modern Theory</h2>

<ul>
<li><b>Bayes Factor:</b> BFâ‚â‚€ = P(Data|Hâ‚)/P(Data|Hâ‚€)</li>
<li><b>Bootstrap:</b> Resampling to estimate distributions</li>
<li><b>Permutation Tests:</b> Exact non-parametric inference</li>
<li><b>Robust Statistics:</b> M-estimators, trimmed means</li>
</ul>

<hr>

<h2>VII. Key Theoretical Insights</h2>

<ul>
<li>All tests are decision rules balancing Type I & Type II errors</li>
<li>Non-parametric tests trade efficiency for robustness</li>
<li>Parametric tests are likelihood-based under strong assumptions</li>
<li>Modern statistics blends frequentist, Bayesian & computational methods</li>
</ul>

<h2>VIII. Conclusion</h2>

<p>
Understanding theoretical foundations ensures correct test selection,
valid inference, and proper interpretation across scientific disciplines.
</p>

</section>
</body>
</html>
